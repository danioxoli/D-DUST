{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% Preprocessing of data\n"
    }
   },
   "source": [
    "# PreProcessing: Feature Selection\n",
    "Feature Selection is an important step in data pre-processing. It consists in selecting the best subset of input variable as the most pertinent. Discarding irrelevant data is essential before applying Machine Learning algorithm in order to:\n",
    "* *Reduce Overfitting*: less opportunity to make decisions based on noise;\n",
    "* *Improve Accuracy*: less misleading data means modelling accuracy improves. Predictions can be greatly distorted by redundant attributes. \n",
    "* *Reduce Training Time*: With less data the algorithms will train faster;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Notebook\n",
    "Before running the notebook, check these parameter value in order to configure dataset and method option. In particular, we have:\n",
    "- **DATA_NORMALIZED**: If *True* data will be normalized between (0,1) before feature selection;\n",
    "- **DATA_STANDARDIZED**: If *True* data will be standarized before feature selection;\n",
    "- **RESULTS_NORMALIZED**: If *True* results of feature selection will be normalized between (0,1);\n",
    "- **TARGET_VARIABLE**: It's the target variable used for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_NORMALIZED = False\n",
    "DATA_STANDARDIZED = True\n",
    "RESULTS_NORMALIZED = True\n",
    "TARGET_VARIABLE = 'pm25_int'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "from fs import methods as m\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import\n",
    "Data are imported from .gpkg file, splitted in **train_set** and **test_set** which are used by supervised algorithm. Otherwise with unsupervised algorithm the entire dataset will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#read gpkg file\n",
    "data = gpd.read_file('grids/test_grid.gpkg')\n",
    "#read variables which are not null\n",
    "labels = m.check_NotNull(pd.DataFrame(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of dataframe used for feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data, columns=labels)\n",
    "Y = X[[TARGET_VARIABLE]]\n",
    "Y = Y.values.ravel()\n",
    "X.pop(TARGET_VARIABLE)\n",
    "X.pop('geometry')\n",
    "#coordinates definition used for mgwr\n",
    "coords = list(zip(data['lat_cen'], data['lng_cen']))\n",
    "\n",
    "if(DATA_NORMALIZED):\n",
    "    X_notNorm= X\n",
    "    X = m.NormalizeData2D(X)\n",
    "\n",
    "if(DATA_STANDARDIZED):\n",
    "    X_notStand = X\n",
    "    X = X.apply(stats.zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact there isn’t a best feature selection technique, many different methods are performed. The aim of this part is to discover by experimentation which one/ones work better for this specific problem.\n",
    "In this study, I choose supervised methods, which are classified into 3 groups, based on their  different approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods\n",
    "Filter-based feature selection methods adopt statistical measures to evaluate the correlation/dependence between input variables.\n",
    "These select features from the without machine learning algorithm. In terms of computation, they are very fast and are very suitable in order to remove duplicated, correlated, redundant variables. On the contrary,  these methods do not remove multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation index\n",
    "It's a measure of linear correlation which stands for the ratio between the covariance of two variables and the product of their standard deviations. The algorithm evaluate the index for each feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple = m.pearson(X, Y, RESULTS_NORMALIZED)\n",
    "#results saved in an external .csv\n",
    "tuple.to_csv(r'results/pearson.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearmanr correlation coefficient\n",
    "The Spearman correlation coefficient is a measure of the monotonicity of the relationship between two datasets.\n",
    "The algorithm evaluate the index for each feature variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple =m.spearmanr(X, Y, RESULTS_NORMALIZED)\n",
    "#results saved in an external .csv\n",
    "tuple.to_csv(r'results/spearmanr.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kendall tau\n",
    "As the Spearmanr correlation coefficient, is based on the ranks of data.\n",
    "In most of the situations, the interpretations of Kendall’s tau and Spearman’s rank correlation coefficient are very similar and thus invariably lead to the same inferences.\n",
    "The algorithm evaluate the index for each feature variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple = m.kendall(X, Y, RESULTS_NORMALIZED)\n",
    "#results saved in an external .csv\n",
    "tuple.to_csv(r'results/kendall.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square test not allowed with no categorical input\n",
    "A chi-square test is used to test events independence. In feature selection instead, we aim to select the features which are more highly dependent. Even if is more suitable for categorical input, computation is performed by casting features values as *Int values*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#tuple = m.chi2_test(X, Y)\n",
    "#score_frame.append(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Test\n",
    "It's a statistical test used to compare model from X and Y and check if the difference is significant between them through regression. The algorithm evaluate the Fisher score for each feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple = m.f_test(X, Y, RESULTS_NORMALIZED)\n",
    "#results saved in an external .csv\n",
    "tuple.to_csv(r'results/fisher.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Ratio (Unsupervised)\n",
    "It stands for the ratio between arithmetic mean the geometric mean. This is very usefull to check dispersion on data. Higher dispersion implies a higher value of this coefficent, thus a more relevant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#tuple = m.compute_dispersion_ratio(preprocessing.normalize(X))\n",
    "#score_frame.append(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold (Unsupervised)\n",
    "It's an approach which aims to remove all features which variance doesn’t meet some threshold. Usually it removes all zero-variance features, so variables taht contains useless information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple = m.variance_threshold(X_notStand, RESULTS_NORMALIZED)\n",
    "#results saved in an external .csv\n",
    "tuple.to_csv(r'results/variance_threshold.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods (and Embedded)\n",
    "Wrapper methods, as the name suggests, wrap a machine learning model, with different subsets of input features: In this way the subsets are  evaluated following  the best model performance.\n",
    "Embedded methods instead are characterised by the benefits of both the wrapper and filter methods, by including interactions of features but also having a reasonable computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%Section 2: Wrapped/Embedded methods%\n"
    }
   },
   "source": [
    "### Exhaustive feature selection for regression analysis\n",
    "This algorithm follow the exhaustive feature selection approach with brute-force evaluation of feature subsets; the best subset is selected by optimizing a specified metric given an arbitrary regressor or classifier. In this case a transformer is used to perform the Sequential Feature Selection.\n",
    "The final outputs are:\n",
    "* Accuracy for the subset choosen;\n",
    "* Indices of the choosen features;\n",
    "* Corresponding names of the features choosen;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#m.exhaustive_feature_selection(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest importance\n",
    "It uses a forest of trees to evaluate the importance of each features as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple = m.RF_importance(X, Y,RESULTS_NORMALIZED)\n",
    "#results saved in an external .csv\n",
    "tuple.to_csv(r'results/RF_importance.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Selection\n",
    "The goal of RFE is to select features by recursively considering smaller and smaller sets of features.\n",
    "The final output for each variable will be:\n",
    "* *Label*;\n",
    "* A *boolean expressing* whetever is selected or not;\n",
    "* *Ranking value* based on its score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple = m.recursive_feature_selection(X, Y.astype(int), 5)\n",
    "#results saved in an external .csv\n",
    "tuple.to_csv(r'results/rfe.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Multiscale Geographically Weighted Regression\n",
    "Due to the fact that this study is related to geographic and spatial data, each pieces of data is very sensitive to the geographic distance between them. So the use of mgwr methods could be innovative, since multivariate models are increasingly encountered in geographical research to estimate spatially varying relationships between a targets and its predictive variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Section 3: Multiscale Geographically Weighted Regression\n"
    }
   },
   "outputs": [],
   "source": [
    "#m.mgwr(data, list(X.columns), coords, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-a9067d49",
   "language": "python",
   "display_name": "PyCharm (notebooks)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}